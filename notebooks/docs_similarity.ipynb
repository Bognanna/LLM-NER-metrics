{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bcffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88780ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Docs_similarity():\n",
    "    \n",
    "    def __init__(self):\n",
    "        print('Instance created')\n",
    "\n",
    "    def cosine(v1, v2):\n",
    "        v1, v2 = np.array(v1), np.array(v2)\n",
    "        return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "    \n",
    "    def document_to_avg_embedding(doc, embeddings):\n",
    "        \n",
    "            doc_embeddings = []\n",
    "            doc = str.lower(doc)\n",
    "            tokens = word_tokenize(doc)\n",
    "\n",
    "            for token in tokens:\n",
    "\n",
    "                try:\n",
    "                    if token in embeddings.keys():\n",
    "                        doc_embeddings.append(embeddings[token])\n",
    "                except AttributeError as e:\n",
    "                    if token in embeddings.key_to_index.keys():\n",
    "                        doc_embeddings.append(embeddings[token])\n",
    "            \n",
    "            avg_embedding = np.mean(np.array(doc_embeddings), axis=0)\n",
    "\n",
    "            return avg_embedding\n",
    "        \n",
    "    def cosine_sim_between_docs(self, doc1, doc2, embeddings, verbose=False):\n",
    "\n",
    "        v1 = self.document_to_avg_embedding(doc1, embeddings)\n",
    "        v2 = self.document_to_avg_embedding(doc2, embeddings)\n",
    "\n",
    "        # If there is no embedings for a doc, return 0.0 (a != a returns true for NaNs).\n",
    "        try: any(v1 != v1)\n",
    "        except TypeError:\n",
    "            if verbose: print(f'Warning cannot find embedding for {doc1}.')\n",
    "            return 0.0\n",
    "        \n",
    "        try: any(v2 != v2)\n",
    "        except TypeError:\n",
    "            if verbose: print(f'Warning cannot find embedding for {doc2}.')\n",
    "            return 0.0\n",
    "        \n",
    "        cosine_sim = self.cosine(v1, v2)\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Cosine similarity between {doc1} and {doc2} is:\\n{cosine_sim}\\n')\n",
    "\n",
    "        return cosine_sim\n",
    "    \n",
    "    def calculate_similarity_matrix(cadec_sample, mapping):\n",
    "        similarity_matrix = np.zeros((5,5))\n",
    "        count_matrix = np.zeros((5,5))\n",
    "\n",
    "        for index_i, row_i in cadec_sample.iterrows():\n",
    "            for index_j, row_j in cadec_sample.iterrows():\n",
    "                \n",
    "                # Do not calculate simmilarities for the same entities\n",
    "                # to not overestimate the score within the same group.\n",
    "                if index_i != index_j:\n",
    "                    similarity_matrix[row_i.entity_type][row_j.entity_type] += cosine_sim_between_docs(row_i.text, row_j.text, mapping, verbose=False)\n",
    "                    count_matrix[row_i.entity_type][row_j.entity_type] += 1\n",
    "\n",
    "        similarity_matrix /= count_matrix\n",
    "        return similarity_matrix\n",
    "    \n",
    "    def plot_similarity_matrix(similarity_matrix):\n",
    "    \n",
    "        plt.imshow(similarity_matrix, vmin=-1, vmax=1, extent=[0, 5, 0, 5]) \n",
    "        for i in range(5): \n",
    "            for j in range(5): \n",
    "                plt.annotate(str(round(similarity_matrix[i][j], 3)),\n",
    "                            xy=(j+0.5, i+0.7), \n",
    "                            ha='center', va='center', color='white') \n",
    "                \n",
    "        plt.show()\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
