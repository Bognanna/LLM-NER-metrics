{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccac866",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9077a",
   "metadata": {},
   "source": [
    "### Benchmark dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81efa784",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\\\data\\\\external\\\\benchmark\\\\benchmark.json') as f:\n",
    "    benchmark_dataset = json.load(f)\n",
    "    benchmark_dataset = benchmark_dataset['Data']\n",
    "    f.close()\n",
    "\n",
    "sentences = []\n",
    "perfect = []\n",
    "good = []\n",
    "ok = []\n",
    "\n",
    "for element in benchmark_dataset:\n",
    "    sentences.append(element['Sentence'])\n",
    "    perfect.append(element['Perfect'])\n",
    "    good.append(element['Good'])\n",
    "    ok.append(element['Ok'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb72cbf4",
   "metadata": {},
   "source": [
    "## Get text embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b6a2c",
   "metadata": {},
   "source": [
    "### Using Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5952528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "EMBED_MODEL = 'nomic-embed-text'\n",
    "\n",
    "def get_text_embedding_using_ollama(text:str):\n",
    "    \n",
    "    if text:\n",
    "\n",
    "        response = ollama.embed(\n",
    "            model=EMBED_MODEL,\n",
    "            input=text,\n",
    "        )\n",
    "\n",
    "        return response[\"embeddings\"]\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc69364f",
   "metadata": {},
   "source": [
    "## Approach 1: Measure quality of generated by LLM entities with their labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e527c21b",
   "metadata": {},
   "source": [
    "\n",
    "~~For each pairwise matching between generated entities and gold entities the sum of cosinus similarities between their embeddings is calculated. Similarity for each entity without a pair is -1. As a metric, the maximal sum of similarities is taken and divided by max(number_of_generated_entites, number_of_gold_entities).~~"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff689f89",
   "metadata": {},
   "source": [
    "### Metric 1.1: Maximal average cosinus similarity of embeddings\n",
    "The metric consists in fact of several metrics:\n",
    "\n",
    "1) \n",
    "- a) Maximal average cosinus similarity of embeddings (brute force)\n",
    "For each pairwise matching between generated entities and gold entities the sum of cosinus similarities between their embeddings is calculated. As a metric, the max sum of similarities is taken and divided by min(number_of_generated_entites, number_of_gold_entities). \n",
    "\n",
    "- b) Average cosinus similarity of embeddings (greedy)\n",
    "For greedy matching between generated entities and gold entities the sum of cosinus similarities between their embeddings is calculated. The sum is divided by min(number_of_generated_entites, number_of_gold_entities).\n",
    "\n",
    "2) Found entities measure\n",
    "- | generated entities | / | gold entities | - 1\n",
    "- <-1; 0) - LLM did not found all entities\n",
    "- 0 - LLM found all entities\n",
    "- (0; +oo) - LLM halucynated some entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf1e384",
   "metadata": {},
   "source": [
    "#### Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d395435a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np   \n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    v1 = np.squeeze(np.asarray(v1))\n",
    "    v2 = np.squeeze(np.asarray(v2))\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67deb7",
   "metadata": {},
   "source": [
    "#### Test metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "064a4aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_metric(metric_fun, sentences, perfect, good, ok, verbose = False):\n",
    "\n",
    "    n_passed = 0\n",
    "    n_failed = 0\n",
    "\n",
    "    for s, p, g, o in zip(sentences, perfect, good, ok):\n",
    "\n",
    "        s1 = metric_fun(p, p)\n",
    "        s2 = metric_fun(p, g)\n",
    "        s3 = metric_fun(p, o)\n",
    "\n",
    "        # if verbose: print(f'{p}\\t{g}\\t{o}')\n",
    "        if s1 >= s2 >= s3:\n",
    "            # if verbose: print(f'TEST PASSED\\t{s1}\\t{s2}\\t{s3}\\n')\n",
    "            n_passed += 1\n",
    "        else:\n",
    "            if verbose: print(f'{p}\\t{g}\\t{o}')\n",
    "            if verbose: print(f'TEST FAILED\\t{s1}\\t{s2}\\t{s3}\\n')\n",
    "            n_failed += 1\n",
    "\n",
    "    return n_passed, n_failed\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018a8e58",
   "metadata": {},
   "source": [
    "#### 1.1.1.a Maximal average cosinus similarity of embeddings (brute force)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9ab38bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import copy\n",
    "\n",
    "def max_avg_cos_similarity(generated_entities, golden_entities):\n",
    "\n",
    "    generated_entities = copy.copy(generated_entities)\n",
    "    golden_entities = copy.copy(golden_entities)\n",
    "\n",
    "    min_len = min(len(golden_entities), len(generated_entities))\n",
    "    max_len = max(len(golden_entities), len(generated_entities))\n",
    "    \n",
    "    # Fill list with padding, to obtain two lists of the same size\n",
    "    for n in range(len(golden_entities), max_len):\n",
    "        golden_entities.append(None)\n",
    "\n",
    "    for n in range(len(generated_entities), max_len):  \n",
    "        generated_entities.append(None)\n",
    "        \n",
    "    # Get text embeddings for each entity\n",
    "    golden_embeddings = [get_text_embedding_using_ollama(entity) for entity in golden_entities]\n",
    "    generated_embeddings = [get_text_embedding_using_ollama(entity) for entity in generated_entities]\n",
    "    \n",
    "\n",
    "    best_permutation = generated_entities\n",
    "    maximal_similarity = -1\n",
    "\n",
    "    for permutation in list(itertools.permutations(list(range(max_len)))):\n",
    "\n",
    "        sum_cosine_similarities = 0\n",
    "        permutated_generated_embeddings = [generated_embeddings[i] for i in permutation]\n",
    "\n",
    "        for golden_embedding, generated_embedding in zip(golden_embeddings, permutated_generated_embeddings):\n",
    "\n",
    "            if golden_embedding and generated_embedding:\n",
    "                sum_cosine_similarities += cosine_similarity(golden_embedding, generated_embedding)\n",
    "\n",
    "        if sum_cosine_similarities/min_len > maximal_similarity:\n",
    "            maximal_similarity = sum_cosine_similarities/min_len\n",
    "            best_permutation = [generated_entities[i] for i in permutation]\n",
    "\n",
    "    return maximal_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52cb0bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['severe pain in the muscles in the shoulder area']\t['severe pain in the muscles']\t['severe pain in the muscles', 'severe pain in the shoulder area']\n",
      "TEST FAILED\t1.0\t0.8627380522048517\t0.9574064207803092\n",
      "\n",
      "['increased blood pressure', 'increased heart rate']\t['blood pressure', 'heart rate']\t['increased blood pressure and heart rate']\n",
      "TEST FAILED\t1.0\t0.8590391771065524\t0.9162281472371826\n",
      "\n",
      "['mild shortness of breath on exertion']\t['shortness of breath']\t['breath on exertion']\n",
      "TEST FAILED\t1.0000000000000002\t0.8685195001479415\t0.8846911153655715\n",
      "\n",
      "['left leg swelling', 'left leg redness']\t['leg swelling', 'redness']\t['leg swelling and redness']\n",
      "TEST FAILED\t1.0\t0.8519824239361447\t0.8553893403675195\n",
      "\n",
      "['upper abdominal pain', 'upper abdominal bloating']\t['abdominal pain', 'bloating']\t['abdominal pain and bloating']\n",
      "TEST FAILED\t1.0\t0.833672914990464\t0.8464600754327717\n",
      "\n",
      "['joint stiffness in the morning']\t['joint stiffness']\t['stiffness in morning']\n",
      "TEST FAILED\t1.0000000000000002\t0.8273280305572597\t0.925218796683792\n",
      "\n",
      "['difficulty swallowing', 'difficulty speaking']\t['swallowing', 'speaking difficulties']\t['difficulty swallowing and speaking']\n",
      "TEST FAILED\t1.0\t0.9241474738527136\t0.9305288462469052\n",
      "\n",
      "['high cholesterol', 'high triglycerides']\t['cholesterol', 'triglycerides']\t['high cholesterol and triglycerides']\n",
      "TEST FAILED\t1.0\t0.8933068880626787\t0.9077254993145528\n",
      "\n",
      "['elevated blood glucose levels']\t['blood glucose']\t['glucose levels elevated']\n",
      "TEST FAILED\t1.0\t0.8238225585259946\t0.9614886074059923\n",
      "\n",
      "['mild anemia', 'low hematocrit']\t['anemia', 'hematocrit']\t['mild anemia and low hematocrit']\n",
      "TEST FAILED\t1.0\t0.8773180779249135\t0.8809624535590347\n",
      "\n",
      "['hearing loss in left ear']\t['hearing loss']\t['left ear loss']\n",
      "TEST FAILED\t0.9999999999999998\t0.792186387316653\t0.9260162490366884\n",
      "\n",
      "['pain in right knee', 'swelling in right knee']\t['knee pain', 'swelling']\t['pain and swelling in right knee']\n",
      "TEST FAILED\t1.0\t0.7750880172033825\t0.974072042783425\n",
      "\n",
      "['shortness of breath', 'chest tightness']\t['breathlessness', 'tightness']\t['shortness of breath and chest tightness']\n",
      "TEST FAILED\t1.0\t0.8085583218814065\t0.8810306323212372\n",
      "\n",
      "['swelling of eyelids', 'redness of eyelids']\t['swelling', 'eyelid redness']\t['swelling and redness eyelids']\n",
      "TEST FAILED\t1.0\t0.8448298294323597\t0.9105959094829851\n",
      "\n",
      "['pain in both knees', 'stiffness in both knees']\t['knee pain', 'knee stiffness']\t['pain and stiffness knees']\n",
      "TEST FAILED\t1.0\t0.8862994805886395\t0.904280106395286\n",
      "\n",
      "['occasional headaches around temples']\t['occasional headaches']\t['headaches temples']\n",
      "TEST FAILED\t1.0\t0.817992908901703\t0.9175392959274749\n",
      "\n",
      "['sharp pain in left ear']\t['ear pain']\t['left ear pain']\n",
      "TEST FAILED\t1.0000000000000002\t0.8154492393229246\t0.93033803559453\n",
      "\n",
      "['persistent nasal congestion', 'sinus pressure']\t['nasal congestion', 'pressure']\t['nasal congestion pressure']\n",
      "TEST FAILED\t1.0\t0.836467028224239\t0.8524438740059637\n",
      "\n",
      "['increased thirst', 'frequent urination']\t['thirst', 'urination']\t['increased thirst frequent urination']\n",
      "TEST FAILED\t0.9999999999999999\t0.8732932392642843\t0.8937780712057067\n",
      "\n",
      "['difficulty swallowing', 'sore throat']\t['swallowing', 'throat soreness']\t['difficulty swallowing throat']\n",
      "TEST FAILED\t1.0\t0.914777003664028\t0.9195778140517433\n",
      "\n",
      "['red, watery eyes', 'nasal discharge']\t['watery eyes', 'discharge']\t['red eyes discharge']\n",
      "TEST FAILED\t1.0\t0.8037132982017499\t0.8098544126115812\n",
      "\n",
      "['bloated stomach', 'gassy stomach']\t['bloating', 'stomach']\t['bloated gassy stomach']\n",
      "TEST FAILED\t1.0000000000000002\t0.8612145761468455\t0.9352333402112961\n",
      "\n",
      "['painful burning sensation in chest']\t['burning chest pain']\t['burning sensation chest']\n",
      "TEST FAILED\t1.0000000000000002\t0.9409129031159148\t0.9498155675846762\n",
      "\n",
      "TESTS PASSED: 89\t TESTS FAILED: 23\n"
     ]
    }
   ],
   "source": [
    "p, f = test_metric(max_avg_cos_similarity, sentences, perfect, good, ok, verbose=True)\n",
    "print(f'TESTS PASSED: {p}\\t TESTS FAILED: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44495f4d",
   "metadata": {},
   "source": [
    "#### 1.1.1.b Average cosinus similarity of embeddings (greedy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "223c5f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_idx(x):\n",
    "    k = x.argmax()\n",
    "    ncol = x.shape[1]\n",
    "    return int(k/ncol), int(k%ncol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b49f0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def avg_cos_similarity(generated_entities, golden_entities):\n",
    "\n",
    "    generated_entities = copy.copy(generated_entities)\n",
    "    golden_entities = copy.copy(golden_entities)\n",
    "\n",
    "    min_len = min(len(golden_entities), len(generated_entities))\n",
    "    max_len = max(len(golden_entities), len(generated_entities))\n",
    "    \n",
    "    # Fill list with padding, to obtain two lists of the same size\n",
    "    for n in range(len(golden_entities), max_len):\n",
    "        golden_entities.append(None)\n",
    "\n",
    "    for n in range(len(generated_entities), max_len):  \n",
    "        generated_entities.append(None)\n",
    "        \n",
    "    # Get text embeddings for each entity\n",
    "    golden_embeddings = [get_text_embedding_using_ollama(entity) for entity in golden_entities]\n",
    "    generated_embeddings = [get_text_embedding_using_ollama(entity) for entity in generated_entities]\n",
    "\n",
    "    # Create matrix of calculated similarities between golden and generated embeddings\n",
    "    try:\n",
    "        arr = np.array([[ cosine_similarity(i,j) for i in golden_embeddings] for j in generated_embeddings])\n",
    "    except TypeError:   # if golden_embeddings or generated_embeddings is empty\n",
    "        arr = np.array([[]]) \n",
    "\n",
    "    greedy_similarity = -1\n",
    "\n",
    "    while arr.any():\n",
    "        max_i, max_j = find_max_idx(arr)\n",
    "        greedy_similarity += arr[max_i, max_j]\n",
    "        arr = np.delete(arr, max_i, axis=0)\n",
    "        arr = np.delete(arr, max_j, axis=1)\n",
    "\n",
    "    return greedy_similarity/min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e2e72879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTS PASSED: 102\t TESTS FAILED: 10\n"
     ]
    }
   ],
   "source": [
    "p, f = test_metric(avg_cos_similarity, sentences, perfect, good, ok)\n",
    "print(f'TESTS PASSED: {p}\\t TESTS FAILED: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575f286e",
   "metadata": {},
   "source": [
    "#### 1.1.2 Found entities measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5bded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def found_entities(generated_entities, golden_entities):\n",
    "    # <-1; 0) - LLM did not found all entities\n",
    "    # 0 - LLM found all entities\n",
    "    # (0; +oo) - LLM halucynated some entities\n",
    "    return len(generated_entities)/len(golden_entities) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "036cd880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTS PASSED: 57\t TESTS FAILED: 55\n"
     ]
    }
   ],
   "source": [
    "p, f = test_metric(found_entities, sentences, perfect, good, ok)\n",
    "print(f'TESTS PASSED: {p}\\t TESTS FAILED: {f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75fe5e",
   "metadata": {},
   "source": [
    "## Approach 2: Measure accuracy (or other metric) of labeling task performed by LLM on each word in a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748bfcdc",
   "metadata": {},
   "source": [
    "### Metric 2.1: Accuracy of generated labels\n",
    "For each generated label compare it to gold label and calculate number of correct generated labels, then divide it by a number of words in a sentence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
